{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from itertools import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score, mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from itertools import chain, combinations\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.cluster import KMeans\n",
    "from clover.over_sampling import ClusterOverSampler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Defining the dataset that is being used by importing it via .csv\n",
    "df = pd.read_csv('/datc/nano/notebooks/Target variable & Features (V3).csv', index_col= 0)\n",
    "df = df[df['Threshold method']=='yen']\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 3\n",
    "\n",
    "# Defining the different features used by the experiment\n",
    "features_used = ['Threshold: area spread', 'Threshold: border', 'Threshold: count', 'Threshold: fill', 'Threshold: intensity', 'Threshold: separation']\n",
    "\n",
    "# Defining the label that has to be predicted\n",
    "pred_label = 'score_label'\n",
    "\n",
    "# Defining the total amount of classes that this run has to execute\n",
    "total_classes_amount = 4\n",
    "\n",
    "# Init combination dataframes\n",
    "df_combinations_dict = {}\n",
    "\n",
    "# Removing warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='macro')\n",
    "    rec = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    diff = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    return (acc, prec, rec, f1, diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model function definitions \n",
    "In this section the function for the different models are defined, please follow the format bellow to make sure it can be used by this program.\n",
    "\n",
    "#### input\n",
    "* X_train\n",
    "* X_test\n",
    "* y_train\n",
    "* y_test\n",
    "\n",
    "#### output\n",
    "* accuracy_score : float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def trainDecisionTree(X_train, X_test, y_train, y_test):\n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\" ,max_depth=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    return score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def trainRandomForestClassifier(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    return score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def trainLogisticRegression(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression(max_iter=5000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def trainMLPClassifier(X_train, X_test, y_train, y_test):\n",
    "    model = MLPClassifier(solver= 'adam', max_iter = 5000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing algorithms function definitions \n",
    "In this section the function for the different balancing algorithms are defined, please follow the format bellow to make sure it can be used by this program.\n",
    "\n",
    "#### input\n",
    "* data : DataFrame (Dataframe that the balancing has to be done over)\n",
    "\n",
    "#### output\n",
    "* df : DataFrame (Returns a DataFrame with the balanced classes. Classes are indexed by 'pred_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def applyNoBalancing(data):\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def applyRandomOversampling(data):\n",
    "    \n",
    "    X = data.drop(pred_label, axis=1)\n",
    "    y = data[pred_label]\n",
    "    ros = RandomOverSampler(random_state=RANDOM_SEED+3)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "    return X_res.join(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def applyClusterBasedOversamplingSMOTE(data):\n",
    "    \n",
    "    X = data.drop(pred_label, axis= 1)\n",
    "    y = data[pred_label]\n",
    "    \n",
    "    # Create KMeans-SMOTE instance\n",
    "    smote = SMOTE()\n",
    "    kmeans = KMeans(n_clusters=8)\n",
    "    kmeans_smote = ClusterOverSampler(oversampler=smote, clusterer=kmeans)\n",
    "    \n",
    "    # Fit and resample imbalanced data\n",
    "    X_res, y_res = kmeans_smote.fit_resample(X, y)\n",
    "    \n",
    "    return X_res.join(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def applySMOTE(data):\n",
    "    sm = SMOTE(k_neighbors=4)\n",
    "    \n",
    "    X = data.drop(pred_label, axis= 1)\n",
    "    y = data[pred_label]\n",
    "    \n",
    "    X_res, y_res = sm.fit_sample(X, y)\n",
    "    \n",
    "    return X_res.join(y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting classses function definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def normalSplitter(data, classes_amount):\n",
    "    class_labels = []\n",
    "\n",
    "    current_classes = np.sort(data['User score'].unique())\n",
    "    split = np.array_split(current_classes, classes_amount) \n",
    "\n",
    "    for row in data.iterrows():\n",
    "        for label, class_ranges in enumerate(split): \n",
    "            if(row[1]['User score'] in class_ranges): \n",
    "                class_labels.append(label) \n",
    "    data['score_label'] = class_labels  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def nonLinearSplitter(data, classes_amount):\n",
    "    score_count = 10\n",
    "    start = 0\n",
    "    \n",
    "    def n_score(score):\n",
    "        return data[data['User score']==score].shape[0]\n",
    "    \n",
    "    n_score_list = []\n",
    "    for n in range(10):\n",
    "        n_score_list.append(n_score(n + 1))\n",
    "\n",
    "    def recursiveFunction(start, classes, scores):\n",
    "        lst_combinations = []\n",
    "        n = scores - start - classes\n",
    "\n",
    "        # N score is 0 so only groups of 1 are possible\n",
    "        if n == 0:\n",
    "            lst = []\n",
    "            for i in range(classes):\n",
    "                lst.append([sum(n_score_list[start+ i:start+i + 1]), list(range(start+ i + 1, start+i + 2 ))])\n",
    "            lst_combinations.append(lst)\n",
    "\n",
    "        # Only one class is left so group remaining scores\n",
    "        elif classes == 1:\n",
    "            return [[[sum(n_score_list[start:]), list(range(start + 1,scores + 1))]]]\n",
    "\n",
    "        # Loop over range of N score (Recursive part)\n",
    "        else:\n",
    "            for m in range(n + 1):\n",
    "                sub_lst = recursiveFunction(start + n - m +1, classes -1, scores)    \n",
    "                for k in sub_lst:\n",
    "                    lst = [[sum(n_score_list[start:start + n-m + 1]), list(range(start + 1, start + n-m + 2))]]\n",
    "                    lst.extend(k)\n",
    "                    lst_combinations.append(lst)\n",
    "        return lst_combinations\n",
    "\n",
    "\n",
    "    best_average = -1\n",
    "    combinations = recursiveFunction(start,classes_amount, score_count)\n",
    "\n",
    "    for cb in combinations:\n",
    "        if best_average == -1:\n",
    "            best_average = cb\n",
    "        else:\n",
    "            best_average_scores = []\n",
    "            cb_scores = []\n",
    "            for score in best_average:\n",
    "                best_average_scores.append(score[0])\n",
    "            for score in cb:\n",
    "                cb_scores.append(score[0])\n",
    "            if max(cb_scores) - min(cb_scores) < max(best_average_scores) - min(best_average_scores):\n",
    "                best_average = cb\n",
    "\n",
    "    cur_label = 0\n",
    "    data[pred_label] = 0\n",
    "    for score in best_average:\n",
    "        mask = data['User score'].isin(score[1])\n",
    "        data.loc[mask, pred_label] = cur_label\n",
    "        cur_label += 1\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generateScoresDataframe(model, balancing_method):\n",
    "    d_acc = {'score_type':'accuracy'}\n",
    "    d_rec = {'score_type':'recall'}\n",
    "    d_prec = {'score_type':'precision'}\n",
    "    d_f1 = {'score_type':'f1'}\n",
    "    d_diff = {'score_type': 'difference'}\n",
    "    progress = 0.0\n",
    "    progress_string = f'%-40s Balancing method: {balancing_method}' % f'Model: {model}'\n",
    "    \n",
    "    # Define scaler used to scale the classes\n",
    "    scaler = PowerTransformer(method='box-cox')\n",
    "\n",
    "    # Loop over all possible class amounts and feature combinations\n",
    "    for amount in range(total_classes_amount):\n",
    "        \n",
    "        accuracy_list = []\n",
    "        recall_list = []\n",
    "        precision_list = []\n",
    "        f1_list = []\n",
    "        diff_list = []\n",
    "        \n",
    "        # Create all subsets for the features\n",
    "        subsets_features = list(chain(*map(lambda x: combinations(features_used, x), range(1, len(features_used)+1))))\n",
    "        \n",
    "        # Apply non linear scaling when dividing classes\n",
    "        split_df = class_splitter(df, amount + 2)\n",
    "        split_df = split_df.drop(['User score', 'Threshold method'], axis=1)\n",
    "        \n",
    "        np_array_acc = np.array([])\n",
    "        np_array_rec = np.array([])\n",
    "        np_array_prec = np.array([])\n",
    "        np_array_f1 = np.array([])\n",
    "        \n",
    "        split_df_cross = np.array_split(split_df, 5)\n",
    "        \n",
    "        for n_split, test_scaled_df in enumerate(split_df_cross, start=0):\n",
    "\n",
    "            acc_l_temp = []\n",
    "            rec_l_temp = []\n",
    "            prec_l_temp = []\n",
    "            f1_l_temp = []\n",
    "            diff_l_temp = []\n",
    "            \n",
    "            train_scaled_df = pd.concat([split_df, test_scaled_df]).drop_duplicates(keep= False)\n",
    "\n",
    "            # Apply balancing method to training set\n",
    "            train_balanced_scaled_df = balancing_dict[balancing_method](train_scaled_df)\n",
    "\n",
    "            i = 0\n",
    "            for subset in subsets_features:\n",
    "                i += 1\n",
    "                \n",
    "                # dataframe to train on\n",
    "                X_train = train_balanced_scaled_df[list(subset)]\n",
    "                y_train = train_balanced_scaled_df[pred_label]\n",
    "\n",
    "                # dataframe to test on\n",
    "                X_test = test_scaled_df[list(subset)]\n",
    "                y_test = test_scaled_df[pred_label]\n",
    "                \n",
    "                # Scaling of dataset features\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "\n",
    "                # Call the method linked to model key to get the scores\n",
    "                scores = models_dict[model](X_train, X_test, y_train, y_test)\n",
    "                acc_l_temp.append(scores[0])\n",
    "                prec_l_temp.append(scores[1])\n",
    "                rec_l_temp.append(scores[2])\n",
    "                f1_l_temp.append(scores[3])\n",
    "                diff_l_temp.append(scores[4])\n",
    "\n",
    "                progress = round(((i/len(list(subsets_features))*.2 + n_split/5) * (1 / total_classes_amount) + (amount / total_classes_amount)) * 100 ,1)\n",
    "                print(f'%-100s {progress}%% done' % progress_string, end='\\r')\n",
    "                \n",
    "            # Create numpy array on first itteration\n",
    "            if(n_split == 0):\n",
    "                np_array_acc = np.array([acc_l_temp])\n",
    "                np_array_prec = np.array([prec_l_temp])\n",
    "                np_array_rec = np.array([rec_l_temp])\n",
    "                np_array_f1 = np.array([f1_l_temp])\n",
    "                np_array_diff = np.array([diff_l_temp])\n",
    "            else:\n",
    "                np_array_acc = np.append(np_array_acc, [acc_l_temp], axis=0)\n",
    "                np_array_prec = np.append(np_array_prec, [prec_l_temp],axis=0)\n",
    "                np_array_rec = np.append(np_array_rec, [rec_l_temp],axis=0)\n",
    "                np_array_f1 = np.append(np_array_f1, [f1_l_temp],axis=0)\n",
    "                np_array_diff = np.append(np_array_diff, [diff_l_temp],axis=0)\n",
    "        \n",
    "        # Save accuracy list to dict we use to create the dataframe      \n",
    "        d_acc['n_classes=' + str(amount + 2)] = np.average(np_array_acc, axis=0)\n",
    "        d_prec['n_classes=' + str(amount + 2)] = np.average(np_array_prec, axis=0)\n",
    "        d_rec['n_classes=' + str(amount + 2)] = np.average(np_array_rec, axis=0)   \n",
    "        d_f1['n_classes=' + str(amount + 2)] = np.average(np_array_f1, axis=0)\n",
    "        d_diff['n_classes=' + str(amount + 2)] = np.average(np_array_diff, axis=0)\n",
    "    \n",
    "    \n",
    "    print(f'%-100s 100.0%% done' % (progress_string))\n",
    "    subset_labels = []\n",
    "    \n",
    "    # Create labels for feature combinations\n",
    "    for subset in subsets_features:\n",
    "        subset_label = ''\n",
    "        for i in range(len(subset)):\n",
    "            subset_label += subset[i][11]\n",
    "        subset_labels.append(subset_label)\n",
    "    \n",
    "    dataframe = pd.DataFrame()\n",
    "\n",
    "    df_acc = pd.DataFrame(d_acc)\n",
    "    df_acc['features_used'] = subset_labels\n",
    "    df_prec = pd.DataFrame(d_prec)\n",
    "    df_prec['features_used'] = subset_labels\n",
    "    df_rec = pd.DataFrame(d_rec)\n",
    "    df_rec['features_used'] = subset_labels\n",
    "    df_f1 = pd.DataFrame(d_f1)\n",
    "    df_f1['features_used'] = subset_labels\n",
    "    df_diff = pd.DataFrame(d_diff)\n",
    "    df_diff['features_used'] = subset_labels\n",
    "    \n",
    "    dataframe = df_acc.append(df_prec, ignore_index=True)\n",
    "    dataframe = dataframe.append(df_rec, ignore_index=True)\n",
    "    dataframe = dataframe.append(df_f1, ignore_index=True)\n",
    "    dataframe = dataframe.append(df_diff, ignore_index=True)\n",
    "    \n",
    "    dataframe = dataframe.set_index(['score_type','features_used'])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define the models and balancing methods that we make combinations with\n",
    "\n",
    "models_dict = {\n",
    "    'Decision_Tree': trainDecisionTree,\n",
    "    'Logistic_Regression': trainLogisticRegression,\n",
    "    'MLP_Classifier': trainMLPClassifier,\n",
    "    'Random_Forest_Classifier': trainRandomForestClassifier\n",
    "    \n",
    "}\n",
    "\n",
    "# Defining the Balancing Algorithms by storing them as functions in a dict\n",
    "balancing_dict = {\n",
    "    'No-balancing': applyNoBalancing,\n",
    "    'Random-oversampling': applyRandomOversampling,\n",
    "    'SMOTE' : applySMOTE,\n",
    "    'Cluster-based_Oversampling_SMOTE' : applyClusterBasedOversamplingSMOTE\n",
    "}\n",
    "\n",
    "# Define the class splitter used\n",
    "class_splitter = normalSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the Scores DataFrames\n",
      "\n",
      "Model: Decision_Tree                     Balancing method: No-balancing                              100.0% done\n",
      "Model: Decision_Tree                     Balancing method: Random-oversampling                       100.0% done\n",
      "Model: Decision_Tree                     Balancing method: SMOTE                                     100.0% done\n",
      "Model: Decision_Tree                     Balancing method: Cluster-based_Oversampling_SMOTE          100.0% done\n",
      "Model: Logistic_Regression               Balancing method: No-balancing                              100.0% done\n",
      "Model: Logistic_Regression               Balancing method: Random-oversampling                       100.0% done\n",
      "Model: Logistic_Regression               Balancing method: SMOTE                                     100.0% done\n",
      "Model: Logistic_Regression               Balancing method: Cluster-based_Oversampling_SMOTE          100.0% done\n",
      "Model: MLP_Classifier                    Balancing method: No-balancing                              100.0% done\n",
      "Model: MLP_Classifier                    Balancing method: Random-oversampling                       100.0% done\n",
      "Model: MLP_Classifier                    Balancing method: SMOTE                                     100.0% done\n",
      "Model: MLP_Classifier                    Balancing method: Cluster-based_Oversampling_SMOTE          100.0% done\n",
      "Model: Random_Forest_Classifier          Balancing method: No-balancing                              100.0% done\n",
      "Model: Random_Forest_Classifier          Balancing method: Random-oversampling                       100.0% done\n",
      "Model: Random_Forest_Classifier          Balancing method: SMOTE                                     100.0% done\n",
      "Model: Random_Forest_Classifier          Balancing method: Cluster-based_Oversampling_SMOTE          100.0% done\n",
      "\n",
      "Program finished\n"
     ]
    }
   ],
   "source": [
    "# Loop over all models defined in the models_dict\n",
    "\n",
    "print('Generating the Scores DataFrames\\n')\n",
    "for key_model in models_dict:\n",
    "    for key_bal in balancing_dict:\n",
    "        scores_df = generateScoresDataframe(key_model, key_bal)\n",
    "        if key_model in df_combinations_dict:\n",
    "            df_combinations_dict[key_model][key_bal] = scores_df\n",
    "        else:\n",
    "            df_combinations_dict[key_model] = {key_bal: scores_df}\n",
    "        \n",
    "print('\\nProgram finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_combinations_dict['Decision_Tree']['No-balancing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Code to export the dataframes to set location\n",
    "location = 'data/nano/Results/DataFrames/'\n",
    "models = ['Decision_Tree', 'Logistic_Regression', 'MLP_Classifier', 'Random_Forest_Classifier']\n",
    "balancing_methods = ['No-balancing', 'Random-oversampling', 'SMOTE', 'Cluster-based_Oversampling_SMOTE']\n",
    "\n",
    "for model in models:\n",
    "    for method in balancing_methods:\n",
    "        df_combinations_dict[model][method].to_csv(f'{model}-{method}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc['accuracy', 'n_classes=2'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc['f1', 'n_classes=2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>n_classes=2</th>\n",
       "      <th>n_classes=3</th>\n",
       "      <th>n_classes=4</th>\n",
       "      <th>n_classes=5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_type</th>\n",
       "      <th>features_used</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">accuracy</th>\n",
       "      <th>a</th>\n",
       "      <td>0.518182</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.210606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.622727</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.157576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.377273</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.190909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.548485</td>\n",
       "      <td>0.322727</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.203030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.539394</td>\n",
       "      <td>0.503030</td>\n",
       "      <td>0.310606</td>\n",
       "      <td>0.189394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">difference</th>\n",
       "      <th>abcis</th>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>1.093939</td>\n",
       "      <td>1.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abfis</th>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.996970</td>\n",
       "      <td>1.603030</td>\n",
       "      <td>1.601515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acfis</th>\n",
       "      <td>0.428788</td>\n",
       "      <td>0.671212</td>\n",
       "      <td>1.206061</td>\n",
       "      <td>1.653030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bcfis</th>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>1.112121</td>\n",
       "      <td>1.601515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abcfis</th>\n",
       "      <td>0.412121</td>\n",
       "      <td>0.737879</td>\n",
       "      <td>1.148485</td>\n",
       "      <td>1.484848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          n_classes=2  n_classes=3  n_classes=4  n_classes=5\n",
       "score_type features_used                                                    \n",
       "accuracy   a                 0.518182     0.430303     0.136364     0.210606\n",
       "           b                 0.622727     0.450000     0.227273     0.157576\n",
       "           c                 0.516667     0.377273     0.383333     0.190909\n",
       "           f                 0.548485     0.322727     0.236364     0.203030\n",
       "           i                 0.539394     0.503030     0.310606     0.189394\n",
       "...                               ...          ...          ...          ...\n",
       "difference abcis             0.445455     0.672727     1.093939     1.568182\n",
       "           abfis             0.463636     0.996970     1.603030     1.601515\n",
       "           acfis             0.428788     0.671212     1.206061     1.653030\n",
       "           bcfis             0.393939     0.890909     1.112121     1.601515\n",
       "           abcfis            0.412121     0.737879     1.148485     1.484848\n",
       "\n",
       "[315 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_classes=2    0.595714\n",
       "n_classes=3    0.393889\n",
       "n_classes=4    0.279762\n",
       "n_classes=5    0.238000\n",
       "Name: (recall, cfis), dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.loc['recall', 'cfis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc['difference'].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
